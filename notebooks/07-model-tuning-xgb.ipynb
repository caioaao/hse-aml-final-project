{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "  Python: 3.8.2 (default, Jul 16 2020, 14:00:26) \n",
      "[GCC 9.3.0]\n",
      "  pandas: 1.1.0\n",
      "  numpy: 1.19.1\n",
      "  seaborn: 0.10.1\n",
      "  sklearn: 0.23.2\n",
      "  altair: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import altair as alt\n",
    "from altair import datum\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.model import tscv\n",
    "\n",
    "%run constants.py\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Versions:\")\n",
    "print(\"  Python: %s\" % sys.version)\n",
    "for module in [pd, np, sns, sklearn, alt]:\n",
    "    print(\"  %s: %s\" %(module.__name__, module.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning pipeline\n",
    "\n",
    "We're not through with feature engineering, but I want have my hyperparameter optimization pipeline ready. To make things simple I'll be using [Optuna](https://optuna.readthedocs.io/en/stable/index.html). It's a automated hyperparameter optimization framework that implements some bayesian algorithms.\n",
    "\n",
    "I could also use random search or a grid search but I thought this would be a nice opportunity to try something different.\n",
    "\n",
    "The first thing we need to do is define the loss function. We'll define one for XGB first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from src.model.metrics import corrected_rmse\n",
    "from optuna import Trial\n",
    "\n",
    "\n",
    "def xgb_feval(y_pred, dtrain):\n",
    "    return 'cRMSE', corrected_rmse(dtrain.get_label(), y_pred)\n",
    "\n",
    "\n",
    "def make_xgb_loss(X_train, y_train, cv_splits, verbose=True):\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    return lambda params: xgb.cv(\n",
    "        params, dtrain, folds=cv_splits, feval=xgb_feval,\n",
    "        maximize=False, verbose_eval=verbose)['test-cRMSE-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it in action. Also notice we're using XGB's built in CV instead of sklearn's `cross_validate` inside this higher level function that returns the actual loss function so we can reuse the `DMatrix` object and reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import df_to_X_y\n",
    "train_set = pd.read_parquet(os.path.join(PROCESSED_DATA_DIR, 'train-set-features-001.parquet'))\n",
    "X_train, y_train = df_to_X_y(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import tscv\n",
    "cv_splits = tscv.split(train_set['date_block_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.59019+0.00458\ttest-rmse:2.42555+0.21618\ttrain-cRMSE:1.23558+0.00224\ttest-cRMSE:1.15701+0.10312\n",
      "[1]\ttrain-rmse:2.36037+0.00452\ttest-rmse:2.22328+0.22623\ttrain-cRMSE:1.12599+0.00210\ttest-cRMSE:1.06053+0.10791\n",
      "[2]\ttrain-rmse:2.23398+0.00368\ttest-rmse:2.11549+0.23570\ttrain-cRMSE:1.06568+0.00176\ttest-cRMSE:1.00911+0.11243\n",
      "[3]\ttrain-rmse:2.16498+0.00338\ttest-rmse:2.05637+0.24291\ttrain-cRMSE:1.03272+0.00164\ttest-cRMSE:0.98091+0.11587\n",
      "[4]\ttrain-rmse:2.12670+0.00308\ttest-rmse:2.02637+0.24968\ttrain-cRMSE:1.01442+0.00151\ttest-cRMSE:0.96660+0.11910\n",
      "[5]\ttrain-rmse:2.10466+0.00353\ttest-rmse:2.00651+0.25183\ttrain-cRMSE:1.00397+0.00170\ttest-cRMSE:0.95712+0.12013\n",
      "[6]\ttrain-rmse:2.09047+0.00312\ttest-rmse:1.99606+0.25456\ttrain-cRMSE:0.99712+0.00151\ttest-cRMSE:0.95214+0.12143\n",
      "[7]\ttrain-rmse:2.07994+0.00369\ttest-rmse:1.98961+0.25619\ttrain-cRMSE:0.99214+0.00175\ttest-cRMSE:0.94906+0.12220\n",
      "[8]\ttrain-rmse:2.07217+0.00439\ttest-rmse:1.98055+0.25324\ttrain-cRMSE:0.98841+0.00209\ttest-cRMSE:0.94474+0.12080\n",
      "[9]\ttrain-rmse:2.06678+0.00375\ttest-rmse:1.97501+0.25433\ttrain-cRMSE:0.98586+0.00178\ttest-cRMSE:0.94210+0.12132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9420986666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = make_xgb_loss(X_train, y_train, cv_splits)\n",
    "l({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's define the objective function we will feed optuna (that's also where we define the state space for our search).\n",
    "\n",
    "Also note we're fixing the number of boost rounds for now. This is a special hyperparameter since we can use early stopping afterwards and get an optimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Trial\n",
    "STATIC_PARAMS = {\"objective\": \"reg:squarederror\",\n",
    "                 \"n_jobs\": -1,\n",
    "                 \"base_score\": 0.5,\n",
    "                 \"scale_pos_weight\": 1}\n",
    "\n",
    "def trial_to_params(trial: Trial):\n",
    "    return {**STATIC_PARAMS,\n",
    "            \"max_depth\": trial.suggest_int('max_depth', 2, 20, 1),\n",
    "            \"subsample\": trial.suggest_discrete_uniform('subsample', .20, 1.00, .01),\n",
    "            \"colsample_bytree\": trial.suggest_discrete_uniform('colsample_bytree', .20, 1., .01),\n",
    "            \"colsample_bylevel\": trial.suggest_discrete_uniform('colsample_bylevel', .20, 1., .01),\n",
    "            \"seed\": trial.suggest_int('seed', 0, 999999),\n",
    "            \"learning_rate\": trial.suggest_uniform('learning_rate', 0.01, 0.15),\n",
    "            \"gamma\": trial.suggest_categorical(\"gamma\", [0, 0, 0, 0, 0, 0.01, 0.1, 0.2, 0.3, 0.5, 1., 10., 100.]),\n",
    "            \"min_child_weight\": trial.suggest_categorical('min_child_weight', [1, 1, 1, 1, 2, 3, 4, 5, 1, 6, 7, 8, 9, 10, 11, 15, 30, 60, 100, 1, 1, 1]),\n",
    "            \"max_delta_step\": trial.suggest_categorical(\"max_delta_step\", [0, 0, 0, 0, 0, 1, 2, 5, 8]),\n",
    "            \"reg_alpha\": trial.suggest_categorical(\"reg_alpha\", [0, 0, 0, 0, 0, 0.00000001, 0.00000005, 0.0000005, 0.000005]),\n",
    "            \"reg_lambda\": trial.suggest_categorical(\"reg_lambda\", [1, 1, 1, 1, 2, 3, 4, 5, 1])}\n",
    "\n",
    "def make_xgb_objective(xgb_loss):\n",
    "    return lambda trial: xgb_loss(trial_to_params(trial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Now all we have to do is feed it to optuna. Here's an example of how that'd work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-27 15:34:16,934] Trial 0 finished with value: 0.9699249999999999 and parameters: {'max_depth': 15, 'subsample': 0.95, 'colsample_bytree': 0.99, 'colsample_bylevel': 0.37, 'seed': 144093, 'learning_rate': 0.124810494766307, 'gamma': 100.0, 'min_child_weight': 2, 'max_delta_step': 0, 'reg_alpha': 0, 'reg_lambda': 3}. Best is trial 0 with value: 0.9699249999999999.\n",
      "[I 2020-08-27 15:36:36,955] Trial 1 finished with value: 1.1400723333333334 and parameters: {'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 0.41000000000000003, 'colsample_bylevel': 0.8200000000000001, 'seed': 298054, 'learning_rate': 0.042448496604582955, 'gamma': 100.0, 'min_child_weight': 1, 'max_delta_step': 0, 'reg_alpha': 1e-08, 'reg_lambda': 1}. Best is trial 1 with value: 1.1400723333333334.\n",
      "[I 2020-08-27 15:38:54,044] Trial 0 finished with value: 1.2238716666666667 and parameters: {'max_depth': 13, 'subsample': 0.75, 'colsample_bytree': 0.99, 'colsample_bylevel': 0.65, 'seed': 275575, 'learning_rate': 0.034549765109830294, 'gamma': 100.0, 'min_child_weight': 60, 'max_delta_step': 2, 'reg_alpha': 5e-06, 'reg_lambda': 1}. Best is trial 1 with value: 0.9834603333333334.\n",
      "[I 2020-08-27 15:39:11,388] Trial 0 finished with value: 1.016278 and parameters: {'max_depth': 18, 'subsample': 0.24000000000000002, 'colsample_bytree': 0.53, 'colsample_bylevel': 0.5900000000000001, 'seed': 235545, 'learning_rate': 0.09267604192580113, 'gamma': 10.0, 'min_child_weight': 60, 'max_delta_step': 0, 'reg_alpha': 0, 'reg_lambda': 1}. Best is trial 0 with value: 1.016278.\n"
     ]
    }
   ],
   "source": [
    "# just making sure GC runs before study so we don't get OOM errors\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "objective = make_xgb_objective(make_xgb_loss(X_train, y_train, cv_splits, verbose=False))\n",
    "study.optimize(objective, n_trials=2, n_jobs=3, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are stored in the study as an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'n_jobs': -1,\n",
       " 'base_score': 0.5,\n",
       " 'scale_pos_weight': 1,\n",
       " 'max_depth': 18,\n",
       " 'subsample': 0.24000000000000002,\n",
       " 'colsample_bytree': 0.53,\n",
       " 'colsample_bylevel': 0.5900000000000001,\n",
       " 'seed': 235545,\n",
       " 'learning_rate': 0.09267604192580113,\n",
       " 'gamma': 10.0,\n",
       " 'min_child_weight': 60,\n",
       " 'max_delta_step': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = {**STATIC_PARAMS,\n",
    "               **study.best_params}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this parameters and find the optimal number of boosting rounds with the early stopping method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tdtrain-rmse:2.87555\tdtest-rmse:2.81223\tdtrain-cRMSE:1.37582\tdtest-cRMSE:1.34150\n",
      "Multiple eval metrics have been passed: 'dtest-cRMSE' will be used for early stopping.\n",
      "\n",
      "Will train until dtest-cRMSE hasn't improved in 40 rounds.\n",
      "[1]\tdtrain-rmse:2.77999\tdtest-rmse:2.71688\tdtrain-cRMSE:1.32925\tdtest-cRMSE:1.29598\n",
      "[2]\tdtrain-rmse:2.68620\tdtest-rmse:2.62598\tdtrain-cRMSE:1.28392\tdtest-cRMSE:1.25262\n",
      "[3]\tdtrain-rmse:2.58686\tdtest-rmse:2.54867\tdtrain-cRMSE:1.23593\tdtest-cRMSE:1.21576\n",
      "[4]\tdtrain-rmse:2.52886\tdtest-rmse:2.48750\tdtrain-cRMSE:1.20778\tdtest-cRMSE:1.18657\n",
      "[5]\tdtrain-rmse:2.45296\tdtest-rmse:2.42516\tdtrain-cRMSE:1.17132\tdtest-cRMSE:1.15682\n",
      "[6]\tdtrain-rmse:2.40156\tdtest-rmse:2.38016\tdtrain-cRMSE:1.14675\tdtest-cRMSE:1.13537\n",
      "[7]\tdtrain-rmse:2.35633\tdtest-rmse:2.34078\tdtrain-cRMSE:1.12532\tdtest-cRMSE:1.11658\n",
      "[8]\tdtrain-rmse:2.31732\tdtest-rmse:2.30800\tdtrain-cRMSE:1.10680\tdtest-cRMSE:1.10095\n",
      "[9]\tdtrain-rmse:2.27001\tdtest-rmse:2.27467\tdtrain-cRMSE:1.08427\tdtest-cRMSE:1.08506\n",
      "[10]\tdtrain-rmse:2.23757\tdtest-rmse:2.24917\tdtrain-cRMSE:1.06879\tdtest-cRMSE:1.07289\n",
      "[11]\tdtrain-rmse:2.21683\tdtest-rmse:2.22934\tdtrain-cRMSE:1.05896\tdtest-cRMSE:1.06343\n",
      "[12]\tdtrain-rmse:2.19383\tdtest-rmse:2.21303\tdtrain-cRMSE:1.04801\tdtest-cRMSE:1.05565\n",
      "[13]\tdtrain-rmse:2.17379\tdtest-rmse:2.20205\tdtrain-cRMSE:1.03846\tdtest-cRMSE:1.05041\n",
      "[14]\tdtrain-rmse:2.14453\tdtest-rmse:2.18351\tdtrain-cRMSE:1.02451\tdtest-cRMSE:1.04156\n",
      "[15]\tdtrain-rmse:2.11788\tdtest-rmse:2.17024\tdtrain-cRMSE:1.01178\tdtest-cRMSE:1.03523\n",
      "[16]\tdtrain-rmse:2.09533\tdtest-rmse:2.16021\tdtrain-cRMSE:1.00102\tdtest-cRMSE:1.03045\n",
      "[17]\tdtrain-rmse:2.07756\tdtest-rmse:2.14672\tdtrain-cRMSE:0.99254\tdtest-cRMSE:1.02401\n",
      "[18]\tdtrain-rmse:2.05799\tdtest-rmse:2.14099\tdtrain-cRMSE:0.98319\tdtest-cRMSE:1.02128\n",
      "[19]\tdtrain-rmse:2.05213\tdtest-rmse:2.13366\tdtrain-cRMSE:0.98039\tdtest-cRMSE:1.01778\n",
      "[20]\tdtrain-rmse:2.04685\tdtest-rmse:2.12976\tdtrain-cRMSE:0.97784\tdtest-cRMSE:1.01592\n",
      "[21]\tdtrain-rmse:2.03574\tdtest-rmse:2.12488\tdtrain-cRMSE:0.97254\tdtest-cRMSE:1.01359\n",
      "[22]\tdtrain-rmse:2.02644\tdtest-rmse:2.12158\tdtrain-cRMSE:0.96807\tdtest-cRMSE:1.01202\n",
      "[23]\tdtrain-rmse:2.01407\tdtest-rmse:2.11860\tdtrain-cRMSE:0.96215\tdtest-cRMSE:1.01059\n",
      "[24]\tdtrain-rmse:2.00167\tdtest-rmse:2.10484\tdtrain-cRMSE:0.95622\tdtest-cRMSE:1.00403\n",
      "[25]\tdtrain-rmse:1.99969\tdtest-rmse:2.10325\tdtrain-cRMSE:0.95526\tdtest-cRMSE:1.00328\n",
      "[26]\tdtrain-rmse:1.98796\tdtest-rmse:2.10288\tdtrain-cRMSE:0.94964\tdtest-cRMSE:1.00310\n",
      "[27]\tdtrain-rmse:1.98460\tdtest-rmse:2.10120\tdtrain-cRMSE:0.94803\tdtest-cRMSE:1.00230\n",
      "[28]\tdtrain-rmse:1.97818\tdtest-rmse:2.09031\tdtrain-cRMSE:0.94495\tdtest-cRMSE:0.99710\n",
      "[29]\tdtrain-rmse:1.97049\tdtest-rmse:2.09351\tdtrain-cRMSE:0.94129\tdtest-cRMSE:0.99863\n",
      "[30]\tdtrain-rmse:1.96779\tdtest-rmse:2.09253\tdtrain-cRMSE:0.93999\tdtest-cRMSE:0.99816\n",
      "[31]\tdtrain-rmse:1.96564\tdtest-rmse:2.08967\tdtrain-cRMSE:0.93896\tdtest-cRMSE:0.99680\n",
      "[32]\tdtrain-rmse:1.96216\tdtest-rmse:2.08935\tdtrain-cRMSE:0.93729\tdtest-cRMSE:0.99664\n",
      "[33]\tdtrain-rmse:1.95503\tdtest-rmse:2.08363\tdtrain-cRMSE:0.93387\tdtest-cRMSE:0.99391\n",
      "[34]\tdtrain-rmse:1.95200\tdtest-rmse:2.07763\tdtrain-cRMSE:0.93243\tdtest-cRMSE:0.99106\n",
      "[35]\tdtrain-rmse:1.95058\tdtest-rmse:2.07165\tdtrain-cRMSE:0.93175\tdtest-cRMSE:0.98820\n",
      "[36]\tdtrain-rmse:1.94985\tdtest-rmse:2.07155\tdtrain-cRMSE:0.93140\tdtest-cRMSE:0.98815\n",
      "[37]\tdtrain-rmse:1.94955\tdtest-rmse:2.07147\tdtrain-cRMSE:0.93127\tdtest-cRMSE:0.98812\n",
      "[38]\tdtrain-rmse:1.94648\tdtest-rmse:2.06651\tdtrain-cRMSE:0.92980\tdtest-cRMSE:0.98575\n",
      "[39]\tdtrain-rmse:1.94091\tdtest-rmse:2.06668\tdtrain-cRMSE:0.92714\tdtest-cRMSE:0.98583\n",
      "[40]\tdtrain-rmse:1.93958\tdtest-rmse:2.06679\tdtrain-cRMSE:0.92650\tdtest-cRMSE:0.98588\n",
      "[41]\tdtrain-rmse:1.93389\tdtest-rmse:2.06757\tdtrain-cRMSE:0.92377\tdtest-cRMSE:0.98625\n",
      "[42]\tdtrain-rmse:1.92929\tdtest-rmse:2.06766\tdtrain-cRMSE:0.92157\tdtest-cRMSE:0.98630\n",
      "[43]\tdtrain-rmse:1.92542\tdtest-rmse:2.06315\tdtrain-cRMSE:0.91973\tdtest-cRMSE:0.98415\n",
      "[44]\tdtrain-rmse:1.92083\tdtest-rmse:2.06854\tdtrain-cRMSE:0.91752\tdtest-cRMSE:0.98672\n",
      "[45]\tdtrain-rmse:1.91999\tdtest-rmse:2.07020\tdtrain-cRMSE:0.91712\tdtest-cRMSE:0.98751\n",
      "[46]\tdtrain-rmse:1.91467\tdtest-rmse:2.07140\tdtrain-cRMSE:0.91458\tdtest-cRMSE:0.98808\n",
      "[47]\tdtrain-rmse:1.91208\tdtest-rmse:2.06977\tdtrain-cRMSE:0.91334\tdtest-cRMSE:0.98730\n",
      "[48]\tdtrain-rmse:1.91088\tdtest-rmse:2.06885\tdtrain-cRMSE:0.91276\tdtest-cRMSE:0.98687\n",
      "[49]\tdtrain-rmse:1.90730\tdtest-rmse:2.06921\tdtrain-cRMSE:0.91104\tdtest-cRMSE:0.98703\n",
      "[50]\tdtrain-rmse:1.90413\tdtest-rmse:2.06866\tdtrain-cRMSE:0.90952\tdtest-cRMSE:0.98677\n",
      "[51]\tdtrain-rmse:1.90213\tdtest-rmse:2.06860\tdtrain-cRMSE:0.90857\tdtest-cRMSE:0.98674\n",
      "[52]\tdtrain-rmse:1.89965\tdtest-rmse:2.05631\tdtrain-cRMSE:0.90738\tdtest-cRMSE:0.98088\n",
      "[53]\tdtrain-rmse:1.89945\tdtest-rmse:2.05627\tdtrain-cRMSE:0.90729\tdtest-cRMSE:0.98086\n",
      "[54]\tdtrain-rmse:1.89882\tdtest-rmse:2.05332\tdtrain-cRMSE:0.90698\tdtest-cRMSE:0.97945\n",
      "[55]\tdtrain-rmse:1.89463\tdtest-rmse:2.05479\tdtrain-cRMSE:0.90499\tdtest-cRMSE:0.98016\n",
      "[56]\tdtrain-rmse:1.89266\tdtest-rmse:2.05362\tdtrain-cRMSE:0.90405\tdtest-cRMSE:0.97960\n",
      "[57]\tdtrain-rmse:1.89117\tdtest-rmse:2.05419\tdtrain-cRMSE:0.90334\tdtest-cRMSE:0.97987\n",
      "[58]\tdtrain-rmse:1.88813\tdtest-rmse:2.05264\tdtrain-cRMSE:0.90188\tdtest-cRMSE:0.97913\n",
      "[59]\tdtrain-rmse:1.88525\tdtest-rmse:2.05499\tdtrain-cRMSE:0.90051\tdtest-cRMSE:0.98025\n",
      "[60]\tdtrain-rmse:1.88254\tdtest-rmse:2.05615\tdtrain-cRMSE:0.89921\tdtest-cRMSE:0.98081\n",
      "[61]\tdtrain-rmse:1.88071\tdtest-rmse:2.05650\tdtrain-cRMSE:0.89834\tdtest-cRMSE:0.98097\n",
      "[62]\tdtrain-rmse:1.87958\tdtest-rmse:2.05748\tdtrain-cRMSE:0.89779\tdtest-cRMSE:0.98144\n",
      "[63]\tdtrain-rmse:1.87895\tdtest-rmse:2.05635\tdtrain-cRMSE:0.89748\tdtest-cRMSE:0.98090\n",
      "[64]\tdtrain-rmse:1.87846\tdtest-rmse:2.05769\tdtrain-cRMSE:0.89725\tdtest-cRMSE:0.98154\n",
      "[65]\tdtrain-rmse:1.87743\tdtest-rmse:2.06038\tdtrain-cRMSE:0.89677\tdtest-cRMSE:0.98282\n",
      "[66]\tdtrain-rmse:1.87656\tdtest-rmse:2.06364\tdtrain-cRMSE:0.89634\tdtest-cRMSE:0.98438\n",
      "[67]\tdtrain-rmse:1.87615\tdtest-rmse:2.06244\tdtrain-cRMSE:0.89615\tdtest-cRMSE:0.98381\n",
      "[68]\tdtrain-rmse:1.87604\tdtest-rmse:2.06254\tdtrain-cRMSE:0.89610\tdtest-cRMSE:0.98386\n",
      "[69]\tdtrain-rmse:1.87579\tdtest-rmse:2.06338\tdtrain-cRMSE:0.89599\tdtest-cRMSE:0.98425\n",
      "[70]\tdtrain-rmse:1.87427\tdtest-rmse:2.06409\tdtrain-cRMSE:0.89526\tdtest-cRMSE:0.98460\n",
      "[71]\tdtrain-rmse:1.87363\tdtest-rmse:2.06258\tdtrain-cRMSE:0.89495\tdtest-cRMSE:0.98387\n",
      "[72]\tdtrain-rmse:1.87327\tdtest-rmse:2.06229\tdtrain-cRMSE:0.89478\tdtest-cRMSE:0.98373\n",
      "[73]\tdtrain-rmse:1.87265\tdtest-rmse:2.06196\tdtrain-cRMSE:0.89448\tdtest-cRMSE:0.98357\n",
      "[74]\tdtrain-rmse:1.87230\tdtest-rmse:2.06189\tdtrain-cRMSE:0.89432\tdtest-cRMSE:0.98355\n",
      "[75]\tdtrain-rmse:1.87104\tdtest-rmse:2.06204\tdtrain-cRMSE:0.89371\tdtest-cRMSE:0.98362\n",
      "[76]\tdtrain-rmse:1.87059\tdtest-rmse:2.06196\tdtrain-cRMSE:0.89350\tdtest-cRMSE:0.98358\n",
      "[77]\tdtrain-rmse:1.87039\tdtest-rmse:2.06143\tdtrain-cRMSE:0.89340\tdtest-cRMSE:0.98332\n",
      "[78]\tdtrain-rmse:1.86952\tdtest-rmse:2.06083\tdtrain-cRMSE:0.89298\tdtest-cRMSE:0.98304\n",
      "[79]\tdtrain-rmse:1.86832\tdtest-rmse:2.06197\tdtrain-cRMSE:0.89240\tdtest-cRMSE:0.98358\n",
      "[80]\tdtrain-rmse:1.86822\tdtest-rmse:2.06202\tdtrain-cRMSE:0.89236\tdtest-cRMSE:0.98361\n",
      "[81]\tdtrain-rmse:1.86721\tdtest-rmse:2.06071\tdtrain-cRMSE:0.89187\tdtest-cRMSE:0.98298\n",
      "[82]\tdtrain-rmse:1.86701\tdtest-rmse:2.06062\tdtrain-cRMSE:0.89178\tdtest-cRMSE:0.98294\n",
      "[83]\tdtrain-rmse:1.86610\tdtest-rmse:2.06031\tdtrain-cRMSE:0.89134\tdtest-cRMSE:0.98279\n",
      "[84]\tdtrain-rmse:1.86523\tdtest-rmse:2.06141\tdtrain-cRMSE:0.89092\tdtest-cRMSE:0.98332\n",
      "[85]\tdtrain-rmse:1.86513\tdtest-rmse:2.06159\tdtrain-cRMSE:0.89087\tdtest-cRMSE:0.98340\n",
      "[86]\tdtrain-rmse:1.86354\tdtest-rmse:2.06188\tdtrain-cRMSE:0.89011\tdtest-cRMSE:0.98354\n",
      "[87]\tdtrain-rmse:1.86331\tdtest-rmse:2.06307\tdtrain-cRMSE:0.89000\tdtest-cRMSE:0.98411\n",
      "[88]\tdtrain-rmse:1.86324\tdtest-rmse:2.06308\tdtrain-cRMSE:0.88997\tdtest-cRMSE:0.98411\n",
      "[89]\tdtrain-rmse:1.86304\tdtest-rmse:2.06310\tdtrain-cRMSE:0.88987\tdtest-cRMSE:0.98412\n",
      "[90]\tdtrain-rmse:1.86227\tdtest-rmse:2.06300\tdtrain-cRMSE:0.88950\tdtest-cRMSE:0.98408\n",
      "[91]\tdtrain-rmse:1.86215\tdtest-rmse:2.06285\tdtrain-cRMSE:0.88944\tdtest-cRMSE:0.98400\n",
      "[92]\tdtrain-rmse:1.86169\tdtest-rmse:2.06177\tdtrain-cRMSE:0.88922\tdtest-cRMSE:0.98349\n",
      "[93]\tdtrain-rmse:1.86165\tdtest-rmse:2.06166\tdtrain-cRMSE:0.88920\tdtest-cRMSE:0.98343\n",
      "[94]\tdtrain-rmse:1.86127\tdtest-rmse:2.06122\tdtrain-cRMSE:0.88902\tdtest-cRMSE:0.98323\n",
      "[95]\tdtrain-rmse:1.86113\tdtest-rmse:2.06149\tdtrain-cRMSE:0.88896\tdtest-cRMSE:0.98335\n",
      "[96]\tdtrain-rmse:1.86054\tdtest-rmse:2.06128\tdtrain-cRMSE:0.88868\tdtest-cRMSE:0.98325\n",
      "[97]\tdtrain-rmse:1.85942\tdtest-rmse:2.06106\tdtrain-cRMSE:0.88814\tdtest-cRMSE:0.98315\n",
      "[98]\tdtrain-rmse:1.85821\tdtest-rmse:2.06165\tdtrain-cRMSE:0.88757\tdtest-cRMSE:0.98343\n",
      "Stopping. Best iteration:\n",
      "[58]\tdtrain-rmse:1.88813\tdtest-rmse:2.05264\tdtrain-cRMSE:0.90188\tdtest-cRMSE:0.97913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_idx, test_idx = tscv.train_test_split(train_set['date_block_num'])\n",
    "dtrain = xgb.DMatrix(X_train[train_idx], y_train[train_idx])\n",
    "dtest = xgb.DMatrix(X_train[test_idx], y_train[test_idx])\n",
    "bst = xgb.train(best_params, dtrain, early_stopping_rounds=40,\n",
    "                num_boost_round=1000, evals=[(dtrain, 'dtrain'), (dtest, 'dtest')],\n",
    "               feval=xgb_feval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.best_ntree_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use this to train the model and generate a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "             colsample_bylevel=0.5900000000000001, colsample_bynode=1,\n",
       "             colsample_bytree=0.53, gamma=10.0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.09267604192580113, max_delta_step=0, max_depth=18,\n",
       "             min_child_weight=60, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=59, n_jobs=-1, num_parallel_tree=1,\n",
       "             random_state=235545, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=235545, subsample=0.24000000000000002, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=bst.best_ntree_limit, **best_params)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import drop_non_features\n",
    "import zipfile\n",
    "with zipfile.ZipFile(os.path.join(RAW_DATA_DIR, 'competitive-data-science-predict-future-sales.zip'), 'r') as datasets_file:\n",
    "    test_set = pd.read_csv(datasets_file.open('test.csv'))\n",
    "test_subset = pd.read_parquet(os.path.join(PROCESSED_DATA_DIR, 'test-subset-features-001.parquet'))\n",
    "\n",
    "X_test = drop_non_features(test_subset).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.submission import submission_from_subset\n",
    "test_subset['item_cnt_month'] = reg.predict(X_test)\n",
    "submission = submission_from_subset(test_subset, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(TMP_DIR, 'xgb-dataset-001-tuning.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Predict Future Sales"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.30M/2.30M [00:35<00:00, 68.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kaggle c submit -f ${TMP_DIR}/xgb-dataset-001-tuning.csv -m 'Experimenting with XGB with date ids and lagged item counts and hyperparameter optimization using optuna' competitive-data-science-predict-future-sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.08553`. It's an improvement over the last one, but I expected more from the CV scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
