{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "  Python: 3.8.2 (default, Jul 16 2020, 14:00:26) \n",
      "[GCC 9.3.0]\n",
      "  pandas: 1.1.2\n",
      "  numpy: 1.19.2\n",
      "  seaborn: 0.11.0\n",
      "  sklearn: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from tqdm.auto import tqdm, trange\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_validate\n",
    "import zipfile\n",
    "\n",
    "from src.model import tscv, ClippedOutputRegressor\n",
    "from src.data import get_feature_cols, df_to_X_y, drop_non_features, add_lagged_features, add_as_features, df_to_X\n",
    "\n",
    "\n",
    "%run constants.py\n",
    "\n",
    "baseline_reg = joblib.load(os.path.join(MODELS_DIR, 'xgb-baseline.model'))\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Versions:\")\n",
    "print(\"  Python: %s\" % sys.version)\n",
    "for module in [pd, np, sns, sklearn]:\n",
    "    print(\"  %s: %s\" %(module.__name__, module.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stacking\n",
    "\n",
    "Let's start stacking stuff. We already have some CV predictions from previous experiments, so let's start this by building a model on top of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_cv_preds = pd.read_parquet(os.path.join(MODEL_OUTPUTS_DIR, 'cv-lgb-features-025.parquet'))\n",
    "xgb_cv_preds = pd.read_parquet(os.path.join(MODEL_OUTPUTS_DIR, 'cv-xgb-features-025.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oof_preds</th>\n",
       "      <th>oof_idx</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>fold_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453406</td>\n",
       "      <td>3266760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257648</td>\n",
       "      <td>3266761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.243354</td>\n",
       "      <td>3266762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147679</td>\n",
       "      <td>3266763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294278</td>\n",
       "      <td>3266764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   oof_preds  oof_idx  item_cnt_month  fold_id\n",
       "0   0.453406  3266760             1.0        0\n",
       "1   0.257648  3266761             1.0        0\n",
       "2   0.243354  3266762             0.0        0\n",
       "3   0.147679  3266763             0.0        0\n",
       "4   0.294278  3266764             1.0        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_cv_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function to combine the predictions. We want to make sure the split used to generate them are the same, so let's encode that into our function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cv_preds(df1, df2, suffixes=('_df1', '_df2')):\n",
    "    df = df1.merge(df2, on=['oof_idx', 'fold_id', 'item_cnt_month'], suffixes=suffixes)\n",
    "    if not (df.shape[0] == df1.shape[0] and df.shape[0] == df2.shape[0]):\n",
    "        raise ValueError(\"CV preds don't align\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cv_preds = combine_cv_preds(lgb_cv_preds, xgb_cv_preds, suffixes=('_lgb', '_xgb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oof_preds_lgb</th>\n",
       "      <th>oof_idx</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>oof_preds_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453406</td>\n",
       "      <td>3266760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.257648</td>\n",
       "      <td>3266761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.272606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.243354</td>\n",
       "      <td>3266762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147679</td>\n",
       "      <td>3266763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294278</td>\n",
       "      <td>3266764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.324377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   oof_preds_lgb  oof_idx  item_cnt_month  fold_id  oof_preds_xgb\n",
       "0       0.453406  3266760             1.0        0       0.514854\n",
       "1       0.257648  3266761             1.0        0       0.272606\n",
       "2       0.243354  3266762             0.0        0       0.285903\n",
       "3       0.147679  3266763             0.0        0       0.139986\n",
       "4       0.294278  3266764             1.0        0       0.324377"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_cv_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try averaging our model outputs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cv_preds['oof_preds_avg'] = (combined_cv_preds['oof_preds_xgb'] + combined_cv_preds['oof_preds_lgb']) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def cv_df_scores(cv_df, ytrue_col='item_cnt_month', ypred_col='oof_preds', n=3):\n",
    "    scores = []\n",
    "    for fold_id in cv_df['fold_id'].unique()[-n:]:\n",
    "        y_true = cv_df[cv_df['fold_id'] == fold_id][ytrue_col].values\n",
    "        y_pred = cv_df[cv_df['fold_id'] == fold_id][ypred_col].values\n",
    "        err = mean_squared_error(y_true, np.clip(y_pred, 0, 20), squared=False)\n",
    "        scores.append(err)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 0.79638\n",
      "LGB: 0.80421\n",
      "Avg LGB XGB: 0.79747\n"
     ]
    }
   ],
   "source": [
    "print('XGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_xgb')))\n",
    "print('LGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_lgb')))\n",
    "print('Avg LGB XGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_avg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is to be expected. We can try a weighted average, but instead of guessing the weights I'll just train a linear regression with l1 penalty and positive weights on this.\n",
    "\n",
    "First let's generate a train and validation set from the CV predictions. Since the data is reduced, let's use just the last month as a validation set. We also need to recalculate our scores here so we have something to compare the stacking results to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 0.77848\n",
      "LGB: 0.78570\n",
      "Avg LGB XGB: 0.77923\n"
     ]
    }
   ],
   "source": [
    "print('XGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_xgb', n=1)))\n",
    "print('LGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_lgb', n=1)))\n",
    "print('Avg LGB XGB: %.5f' % np.mean(cv_df_scores(combined_cv_preds, ypred_col='oof_preds_avg', n=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "def _clipped_rmse(y, ypred):\n",
    "    return mean_squared_error(y, np.clip(ypred, 0, 20), squared=False)\n",
    "clipped_rmse_score = make_scorer(_clipped_rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = add_as_features(combined_cv_preds, ['oof_preds_xgb', 'oof_preds_lgb'])\n",
    "cv_splits = tscv.split(train_set['fold_id'], n=1, window=7, test_months=train_set['fold_id'].unique())\n",
    "X, y = df_to_X_y(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.94893312]),\n",
       " 'score_time': array([0.00551581]),\n",
       " 'test_score': array([-0.77798477])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "reg = ClippedOutputRegressor(Lasso(random_state=123, alpha=1e-12, positive=True, fit_intercept=True, normalize=False))\n",
    "scores = cross_validate(reg, X, y, cv=cv_splits, scoring=clipped_rmse_score, verbose=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small improvement over our best model (XGB).\n",
    "\n",
    "We can also try our baseline XGB on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.3246274]),\n",
       " 'score_time': array([0.57164526]),\n",
       " 'test_score': array([-0.81622036])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validate(baseline_reg, X, y, cv=cv_splits, scoring=clipped_rmse_score, verbose=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That went terrible for some reason. Let's test the tuned XGB here just for fun (I don't expect it to improve the solution that much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  10.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([5.31406832]),\n",
       " 'score_time': array([4.72536635]),\n",
       " 'test_score': array([-0.81720215])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_025 = joblib.load(os.path.join(MODELS_DIR, 'xgb-features-025.model'))\n",
    "scores = cross_validate(xgb_025, X, y, cv=cv_splits, scoring=clipped_rmse_score, verbose=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also bad. I shouldn't be doing this, but I want to investigate why XGB gives very poor results here. Since the dataset is a lot smaller, let's try changing the tree method to `exact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([7.09215951]),\n",
       " 'score_time': array([0.07775664]),\n",
       " 'test_score': array([-0.7784614])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_small = xgb.XGBRegressor(random_state=8, n_estimators=10, tree_method='exact', n_jobs=-1)\n",
    "cross_validate(xgb_small, X, y, cv=cv_splits, scoring=clipped_rmse_score, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot better, but still worse than our plain Lasso regressor.\n",
    "\n",
    "Another thing we can try is \"passthrough\", where we just re-add the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_025 = pd.read_parquet(os.path.join(PROCESSED_DATA_DIR, 'train-set-features-025.parquet')).iloc[train_set['oof_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set_025 = df_to_X(train_set_025)\n",
    "del train_set_025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_passthrough = np.concatenate((X, X_train_set_025), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([13.06254673]),\n",
       " 'score_time': array([1.57453036]),\n",
       " 'test_score': array([-0.81288667])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(baseline_reg, X_passthrough, y, cv=cv_splits, scoring='neg_root_mean_squared_error', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8172021453770638"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not look good either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
